{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout,Flatten\n",
    "from keras.optimizers import SGD,Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed  = 1\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df,i):\n",
    "    df_std = df.copy()\n",
    "    # apply the z-score method\n",
    "    for column in df_std.columns[i:]:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "    return df_std\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    #return 1-K.square(r)?\n",
    "    return r\n",
    "\n",
    "def correlation(y_true,y_pred):\n",
    "    corr = tfp.stats.correlation(y_true, y_pred, sample_axis=0, event_axis=None)\n",
    "    return corr\n",
    "\n",
    "def Remove(duplicate): \n",
    "    final_list = [] \n",
    "    for num in duplicate: \n",
    "        if num not in final_list: \n",
    "            final_list.append(num) \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units, input_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles( np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, causal=False, dropout=0.0):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    self.w_query = tf.keras.layers.Dense(d_model)\n",
    "    self.split_reshape_query = tf.keras.layers.Reshape((-1,num_heads,depth))  \n",
    "    self.split_permute_query = tf.keras.layers.Permute((2,1,3))      \n",
    "\n",
    "    self.w_value = tf.keras.layers.Dense(d_model)\n",
    "    self.split_reshape_value = tf.keras.layers.Reshape((-1,num_heads,depth))\n",
    "    self.split_permute_value = tf.keras.layers.Permute((2,1,3))\n",
    "\n",
    "    self.w_key = tf.keras.layers.Dense(d_model)\n",
    "    self.split_reshape_key = tf.keras.layers.Reshape((-1,num_heads,depth))\n",
    "    self.split_permute_key = tf.keras.layers.Permute((2,1,3))\n",
    "\n",
    "    self.attention = tf.keras.layers.Attention(causal=causal)  #dropout=dropout\n",
    "    self.join_permute_attention = tf.keras.layers.Permute((2,1,3))\n",
    "    self.join_reshape_attention = tf.keras.layers.Reshape((-1,d_model))\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def call(self, inputs, mask=None, training=None):\n",
    "    q = inputs[0]\n",
    "    v = inputs[1]\n",
    "    k = inputs[2] if len(inputs) > 2 else v\n",
    "\n",
    "    query = self.w_query(q)\n",
    "    query = self.split_reshape_query(query)    \n",
    "    query = self.split_permute_query(query)                 \n",
    "\n",
    "    value = self.w_value(v)\n",
    "    value = self.split_reshape_value(value)\n",
    "    value = self.split_permute_value(value)\n",
    "\n",
    "    key = self.w_key(k)\n",
    "    key = self.split_reshape_key(key)\n",
    "    key = self.split_permute_key(key)\n",
    "\n",
    "    if mask is not None:\n",
    "      if mask[0] is not None:\n",
    "        mask[0] = tf.keras.layers.Reshape((-1,1))(mask[0])\n",
    "        mask[0] = tf.keras.layers.Permute((2,1))(mask[0])\n",
    "      if mask[1] is not None:\n",
    "        mask[1] = tf.keras.layers.Reshape((-1,1))(mask[1])\n",
    "        mask[1] = tf.keras.layers.Permute((2,1))(mask[1])\n",
    "\n",
    "    attention = self.attention([query, value, key], mask=mask)\n",
    "    attention = self.join_permute_attention(attention)\n",
    "    attention = self.join_reshape_attention(attention)\n",
    "\n",
    "    x = self.dense(attention)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, num_layers = 1, d_model = 24, num_heads = 1, dff = 32, maximum_position_encoding = 10000, dropout = 0.0):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "\n",
    "    #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, mask_zero=True)\n",
    "    self.embedding =Linear(d_model, input_vocab_size)\n",
    "    \n",
    "    self.pos = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.encoder_layers = [ EncoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = 0.0) for _ in range(num_layers)]\n",
    "\n",
    "    #self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "  def call(self, inputs, mask=None, training=None):\n",
    "    x = self.embedding(inputs)\n",
    "    x=tf.reshape(x,[-1,1,d_model])\n",
    "    # positional encoding\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n",
    "    x += self.pos[: , :tf.shape(x)[1], :]\n",
    "\n",
    "    #x = self.dropout(x, training=training)\n",
    "\n",
    "    #Encoder layer\n",
    "    #embedding_mask = self.embedding.compute_mask(inputs)\n",
    "    for encoder_layer in self.encoder_layers:\n",
    "      x = encoder_layer(x, mask = None)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def compute_mask(self, inputs, mask=None):\n",
    "    return self.embedding.compute_mask(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,  d_model, num_heads, dff, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.multi_head_attention =  MultiHeadAttention(d_model, num_heads)\n",
    "    #self.dropout_attention = tf.keras.layers.Dropout(dropout)\n",
    "    self.add_attention = tf.keras.layers.Add()\n",
    "    self.layer_norm_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "    #self.dropout_dense = tf.keras.layers.Dropout(dropout)\n",
    "    self.add_dense = tf.keras.layers.Add()\n",
    "    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "  def call(self, inputs, mask=None, training=None):\n",
    "    # print(mask)\n",
    "    attention = self.multi_head_attention([inputs,inputs,inputs], mask = [mask,mask])\n",
    "    #attention = self.dropout_attention(attention, training = training)\n",
    "    x = self.add_attention([inputs , attention])\n",
    "    x = self.layer_norm_attention(x)\n",
    "\n",
    "    ## Feed Forward\n",
    "    dense = self.dense1(x)\n",
    "    dense = self.dense2(dense)\n",
    "    #dense = self.dropout_dense(dense, training = training)\n",
    "    x = self.add_dense([x , dense])\n",
    "    x = self.layer_norm_dense(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, target_vocab_size, num_layers, d_model, num_heads, dff, maximum_position_encoding = 10000, dropout = 0.0):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "\n",
    "    #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model, mask_zero=True)\n",
    "    self.embedding =Linear(d_model, target_vocab_size)\n",
    "    \n",
    "    self.pos = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.decoder_layers = [ DecoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = 0.0)  for _ in range(num_layers)]\n",
    "\n",
    "    #self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "  def call(self, inputs, mask=None, training=None):\n",
    "    x = self.embedding(inputs[0])\n",
    "    x=tf.reshape(x,[-1,1,d_model])\n",
    "    \n",
    "    # positional encoding\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos[: , :tf.shape(x)[1], :]\n",
    "    #x = self.dropout(x, training=training)\n",
    "\n",
    "    #Decoder layer\n",
    "    #embedding_mask = self.embedding.compute_mask(inputs[0])\n",
    "    for decoder_layer in self.decoder_layers:\n",
    "      x = decoder_layer([x,inputs[1]], mask = [None, None])\n",
    "\n",
    "    return x\n",
    "\n",
    "  # Comment this out if you want to use the masked_loss()\n",
    "  def compute_mask(self, inputs, mask=None):\n",
    "    return self.embedding.compute_mask(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,  d_model, num_heads, dff, dropout = 0.0):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.multi_head_attention1 =  MultiHeadAttention(d_model, num_heads, causal = True)\n",
    "    #self.dropout_attention1 = tf.keras.layers.Dropout(dropout)\n",
    "    self.add_attention1 = tf.keras.layers.Add()\n",
    "    self.layer_norm_attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.multi_head_attention2 =  MultiHeadAttention(d_model, num_heads)\n",
    "    #self.dropout_attention2 = tf.keras.layers.Dropout(dropout)\n",
    "    self.add_attention2 = tf.keras.layers.Add()\n",
    "    self.layer_norm_attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "    #self.dropout_dense = tf.keras.layers.Dropout(dropout)\n",
    "    self.add_dense = tf.keras.layers.Add()\n",
    "    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "  def call(self, inputs, mask=None, training=None):\n",
    "    # print(mask)\n",
    "    attention = self.multi_head_attention1([inputs[0],inputs[0],inputs[0]], mask = [mask[0],mask[0]])\n",
    "    #attention = self.dropout_attention1(attention, training = training)\n",
    "    x = self.add_attention1([inputs[0] , attention])\n",
    "    x = self.layer_norm_attention1(x)\n",
    "    \n",
    "    attention = self.multi_head_attention2([x, inputs[1],inputs[1]], mask = [mask[0],mask[1]])\n",
    "    #attention = self.dropout_attention2(attention, training = training)\n",
    "    x = self.add_attention1([x , attention])\n",
    "    x = self.layer_norm_attention1(x)\n",
    "\n",
    "\n",
    "    ## Feed Forward\n",
    "    dense = self.dense1(x)\n",
    "    dense = self.dense2(dense)\n",
    "    #dense = self.dropout_dense(dense, training = training)\n",
    "    x = self.add_dense([x , dense])\n",
    "    x = self.layer_norm_dense(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_2 (Encoder)             (None, None, 24)     4400        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_1 (Decoder)             (None, None, 24)     6560        input_6[0][0]                    \n",
      "                                                                 encoder_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, None, 2)      50          decoder_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,010\n",
      "Trainable params: 11,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 1\n",
    "d_model = 24\n",
    "dff = 32\n",
    "num_heads = 1\n",
    "dropout_rate = 0\n",
    "\n",
    "# Size of input vocab plus start and end tokens\n",
    "input_vocab_size = 12\n",
    "target_vocab_size = 2\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(None,))\n",
    "target = tf.keras.layers.Input(shape=(None,))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout_rate)\n",
    "decoder = Decoder(target_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout_rate)\n",
    "\n",
    "x = encoder(input)\n",
    "x = decoder([target, x] , mask = encoder.compute_mask(input))\n",
    "x = tf.keras.layers.Dense(target_vocab_size)(x)\n",
    "\n",
    "base_model = tf.keras.models.Model(inputs=[input, target], outputs=x)\n",
    "sgd = tf.keras.optimizers.SGD(lr = 0.05)\n",
    "base_model.compile(loss='mse', optimizer=sgd,metrics=[correlation_coefficient])\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Day', 'Symbol', 'Sector', 'Return1w', 'Return2w', 'Return3w', 'Return',\n",
       "       'PER', 'EV_EBITDA', 'PC1M', 'FCFYield', 'SalesYield', 'PBRatio',\n",
       "       'WCRYield', 'FCFPRatio1YearGrowth', 'TotalYield',\n",
       "       'EVToTotalAssets1YearGrowth', 'Vol', 'Mean_Rev'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyData=pd.read_excel('MyData.xlsx', sheet_name=\"Train1m2\")\n",
    "MyData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 41, 61, 81, 101, 121, 141, 161]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Remove(list(MyData['Day']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N=161\n",
    "elem=\"FinancialServices\"\n",
    "#x_Train=MyData[(MyData['Day']<N)&(MyData['Sector']==elem)][MyData.columns[7:]]\n",
    "x_Train=MyData[MyData['Day']<N][MyData.columns[7:]]\n",
    "x_Train=z_score(x_Train,0)\n",
    "x_Train=x_Train.values\n",
    "#y_Train=MyData[(MyData['Day']<N)&(MyData['Sector']==elem)][[\"Return1w\",\"Return2w\",\"Return3w\",\"Return\"]].values\n",
    "y_Train=MyData[MyData['Day']<N][[\"Return1w\",\"Return2w\",\"Return\"]].values\n",
    "#y_Train=np.c_[np.zeros(y_Train.shape[0]),y_Train]\n",
    "\n",
    "#x_Test=MyData[(MyData['Day']==N)&(MyData['Sector']==elem)][MyData.columns[7:]]\n",
    "x_Test=MyData[MyData['Day']==N][MyData.columns[7:]]\n",
    "x_Test=z_score(x_Test,0)\n",
    "x_Test=x_Test.values\n",
    "#y_Test=MyData[(MyData['Day']==N)&(MyData['Sector']==elem)][[\"Return1w\",\"Return2w\",\"Return3w\",\"Return\"]].values\n",
    "y_Test=MyData[MyData['Day']==N][[\"Return1w\",\"Return2w\",\"Return\"]].values\n",
    "#y_Test=np.c_[np.zeros(y_Test.shape[0]),y_Test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3577, 12), (3577, 3), (511, 12), (511, 3))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_Train.shape,y_Train.shape,x_Test.shape,y_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_Train=y_Train.reshape(-1,1,5)\n",
    "#y_Test=y_Test.reshape(-1,1,5)\n",
    "#y_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3066 samples, validate on 511 samples\n",
      "Epoch 1/150\n",
      "3066/3066 [==============================] - 1s 184us/sample - loss: 0.0065 - correlation_coefficient: 0.6140 - val_loss: 0.0033 - val_correlation_coefficient: 0.6329\n",
      "Epoch 2/150\n",
      "3066/3066 [==============================] - 1s 170us/sample - loss: 0.0065 - correlation_coefficient: 0.6734 - val_loss: 0.0034 - val_correlation_coefficient: 0.6343\n",
      "Epoch 3/150\n",
      "3066/3066 [==============================] - 1s 171us/sample - loss: 0.0065 - correlation_coefficient: 0.6841 - val_loss: 0.0035 - val_correlation_coefficient: -0.6281\n",
      "Epoch 4/150\n",
      "3066/3066 [==============================] - 1s 172us/sample - loss: 0.0065 - correlation_coefficient: 0.6341 - val_loss: 0.0035 - val_correlation_coefficient: 0.6260\n",
      "Epoch 5/150\n",
      "3066/3066 [==============================] - 1s 180us/sample - loss: 0.0066 - correlation_coefficient: 0.4649 - val_loss: 0.0032 - val_correlation_coefficient: 0.6333\n",
      "Epoch 6/150\n",
      "3066/3066 [==============================] - 1s 173us/sample - loss: 0.0065 - correlation_coefficient: 0.6218 - val_loss: 0.0036 - val_correlation_coefficient: 0.6119\n",
      "Epoch 7/150\n",
      "3066/3066 [==============================] - 1s 188us/sample - loss: 0.0065 - correlation_coefficient: 0.5784 - val_loss: 0.0035 - val_correlation_coefficient: 0.6352\n",
      "Epoch 8/150\n",
      "3066/3066 [==============================] - 1s 170us/sample - loss: 0.0065 - correlation_coefficient: 0.7087 - val_loss: 0.0037 - val_correlation_coefficient: 0.6132\n",
      "Epoch 9/150\n",
      "3066/3066 [==============================] - 1s 168us/sample - loss: 0.0065 - correlation_coefficient: 0.7871 - val_loss: 0.0034 - val_correlation_coefficient: 0.6346\n",
      "Epoch 10/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0065 - correlation_coefficient: 0.6770 - val_loss: 0.0033 - val_correlation_coefficient: 0.6338\n",
      "Epoch 11/150\n",
      "3066/3066 [==============================] - 1s 170us/sample - loss: 0.0065 - correlation_coefficient: 0.6304 - val_loss: 0.0033 - val_correlation_coefficient: 0.6346\n",
      "Epoch 12/150\n",
      "3066/3066 [==============================] - 1s 168us/sample - loss: 0.0065 - correlation_coefficient: 0.5913 - val_loss: 0.0034 - val_correlation_coefficient: 0.6347\n",
      "Epoch 13/150\n",
      "3066/3066 [==============================] - 1s 168us/sample - loss: 0.0065 - correlation_coefficient: 0.5898 - val_loss: 0.0033 - val_correlation_coefficient: 0.6230\n",
      "Epoch 14/150\n",
      "3066/3066 [==============================] - 1s 166us/sample - loss: 0.0065 - correlation_coefficient: 0.6204 - val_loss: 0.0033 - val_correlation_coefficient: 0.6340\n",
      "Epoch 15/150\n",
      "3066/3066 [==============================] - 1s 193us/sample - loss: 0.0065 - correlation_coefficient: 0.7937 - val_loss: 0.0034 - val_correlation_coefficient: 0.6303\n",
      "Epoch 16/150\n",
      "3066/3066 [==============================] - 0s 155us/sample - loss: 0.0065 - correlation_coefficient: 0.6049 - val_loss: 0.0033 - val_correlation_coefficient: 0.6278\n",
      "Epoch 17/150\n",
      "3066/3066 [==============================] - 1s 175us/sample - loss: 0.0065 - correlation_coefficient: 0.7116 - val_loss: 0.0035 - val_correlation_coefficient: 0.6329\n",
      "Epoch 18/150\n",
      "3066/3066 [==============================] - 0s 161us/sample - loss: 0.0065 - correlation_coefficient: 0.6066 - val_loss: 0.0033 - val_correlation_coefficient: 0.5942\n",
      "Epoch 19/150\n",
      "3066/3066 [==============================] - 1s 172us/sample - loss: 0.0065 - correlation_coefficient: 0.6866 - val_loss: 0.0033 - val_correlation_coefficient: 0.6292\n",
      "Epoch 20/150\n",
      "3066/3066 [==============================] - 1s 169us/sample - loss: 0.0065 - correlation_coefficient: 0.7752 - val_loss: 0.0033 - val_correlation_coefficient: 0.6260\n",
      "Epoch 21/150\n",
      "3066/3066 [==============================] - 0s 153us/sample - loss: 0.0065 - correlation_coefficient: 0.5774 - val_loss: 0.0036 - val_correlation_coefficient: -0.6349\n",
      "Epoch 22/150\n",
      "3066/3066 [==============================] - 0s 162us/sample - loss: 0.0065 - correlation_coefficient: 0.5482 - val_loss: 0.0035 - val_correlation_coefficient: 0.6324\n",
      "Epoch 23/150\n",
      "3066/3066 [==============================] - 1s 166us/sample - loss: 0.0065 - correlation_coefficient: 0.6475 - val_loss: 0.0042 - val_correlation_coefficient: 0.6281\n",
      "Epoch 24/150\n",
      "3066/3066 [==============================] - 1s 172us/sample - loss: 0.0065 - correlation_coefficient: 0.4986 - val_loss: 0.0033 - val_correlation_coefficient: 0.6296\n",
      "Epoch 25/150\n",
      "3066/3066 [==============================] - 1s 166us/sample - loss: 0.0065 - correlation_coefficient: 0.6413 - val_loss: 0.0033 - val_correlation_coefficient: 0.6323\n",
      "Epoch 26/150\n",
      "3066/3066 [==============================] - 1s 173us/sample - loss: 0.0065 - correlation_coefficient: 0.7240 - val_loss: 0.0033 - val_correlation_coefficient: 0.6334\n",
      "Epoch 27/150\n",
      "3066/3066 [==============================] - 1s 171us/sample - loss: 0.0065 - correlation_coefficient: 0.6830 - val_loss: 0.0035 - val_correlation_coefficient: 0.4936\n",
      "Epoch 28/150\n",
      "3066/3066 [==============================] - 1s 167us/sample - loss: 0.0065 - correlation_coefficient: 0.5574 - val_loss: 0.0033 - val_correlation_coefficient: 0.6342\n",
      "Epoch 29/150\n",
      "3066/3066 [==============================] - 1s 227us/sample - loss: 0.0065 - correlation_coefficient: 0.5985 - val_loss: 0.0035 - val_correlation_coefficient: 0.6173\n",
      "Epoch 30/150\n",
      "3066/3066 [==============================] - 1s 194us/sample - loss: 0.0065 - correlation_coefficient: 0.6850 - val_loss: 0.0039 - val_correlation_coefficient: 0.3919\n",
      "Epoch 31/150\n",
      "3066/3066 [==============================] - 1s 213us/sample - loss: 0.0065 - correlation_coefficient: 0.5947 - val_loss: 0.0033 - val_correlation_coefficient: 0.6349\n",
      "Epoch 32/150\n",
      "3066/3066 [==============================] - 1s 191us/sample - loss: 0.0065 - correlation_coefficient: 0.5942 - val_loss: 0.0034 - val_correlation_coefficient: 0.6329\n",
      "Epoch 33/150\n",
      "3066/3066 [==============================] - 1s 200us/sample - loss: 0.0065 - correlation_coefficient: 0.7353 - val_loss: 0.0033 - val_correlation_coefficient: 0.6204\n",
      "Epoch 34/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.6346 - val_loss: 0.0034 - val_correlation_coefficient: 0.6337\n",
      "Epoch 35/150\n",
      "3066/3066 [==============================] - 1s 182us/sample - loss: 0.0065 - correlation_coefficient: 0.7377 - val_loss: 0.0033 - val_correlation_coefficient: 0.6330\n",
      "Epoch 36/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.7110 - val_loss: 0.0034 - val_correlation_coefficient: 0.6256\n",
      "Epoch 37/150\n",
      "3066/3066 [==============================] - 1s 186us/sample - loss: 0.0065 - correlation_coefficient: 0.6208 - val_loss: 0.0034 - val_correlation_coefficient: 0.6351\n",
      "Epoch 38/150\n",
      "3066/3066 [==============================] - 1s 194us/sample - loss: 0.0065 - correlation_coefficient: 0.5766 - val_loss: 0.0033 - val_correlation_coefficient: 0.6142\n",
      "Epoch 39/150\n",
      "3066/3066 [==============================] - 1s 201us/sample - loss: 0.0065 - correlation_coefficient: 0.7117 - val_loss: 0.0035 - val_correlation_coefficient: -0.6260\n",
      "Epoch 40/150\n",
      "3066/3066 [==============================] - 1s 196us/sample - loss: 0.0065 - correlation_coefficient: 0.7453 - val_loss: 0.0039 - val_correlation_coefficient: 0.6340\n",
      "Epoch 41/150\n",
      "3066/3066 [==============================] - 1s 200us/sample - loss: 0.0065 - correlation_coefficient: 0.6364 - val_loss: 0.0032 - val_correlation_coefficient: 0.6335\n",
      "Epoch 42/150\n",
      "3066/3066 [==============================] - 1s 225us/sample - loss: 0.0066 - correlation_coefficient: 0.6053 - val_loss: 0.0038 - val_correlation_coefficient: -0.6308\n",
      "Epoch 43/150\n",
      "3066/3066 [==============================] - 1s 203us/sample - loss: 0.0065 - correlation_coefficient: 0.6855 - val_loss: 0.0036 - val_correlation_coefficient: 0.6322\n",
      "Epoch 44/150\n",
      "3066/3066 [==============================] - 1s 207us/sample - loss: 0.0064 - correlation_coefficient: 0.6891 - val_loss: 0.0033 - val_correlation_coefficient: 0.6315\n",
      "Epoch 45/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.6112 - val_loss: 0.0037 - val_correlation_coefficient: -0.6303\n",
      "Epoch 46/150\n",
      "3066/3066 [==============================] - 1s 191us/sample - loss: 0.0065 - correlation_coefficient: 0.6362 - val_loss: 0.0033 - val_correlation_coefficient: -0.6238\n",
      "Epoch 47/150\n",
      "3066/3066 [==============================] - 1s 184us/sample - loss: 0.0065 - correlation_coefficient: 0.5166 - val_loss: 0.0041 - val_correlation_coefficient: 0.6352\n",
      "Epoch 48/150\n",
      "3066/3066 [==============================] - 1s 184us/sample - loss: 0.0065 - correlation_coefficient: 0.7066 - val_loss: 0.0033 - val_correlation_coefficient: 0.6329\n",
      "Epoch 49/150\n",
      "3066/3066 [==============================] - 1s 188us/sample - loss: 0.0065 - correlation_coefficient: 0.7251 - val_loss: 0.0033 - val_correlation_coefficient: 0.6339\n",
      "Epoch 50/150\n",
      "3066/3066 [==============================] - 1s 184us/sample - loss: 0.0065 - correlation_coefficient: 0.6811 - val_loss: 0.0032 - val_correlation_coefficient: 0.6301\n",
      "Epoch 51/150\n",
      "3066/3066 [==============================] - 1s 199us/sample - loss: 0.0065 - correlation_coefficient: 0.5728 - val_loss: 0.0035 - val_correlation_coefficient: 0.6330\n",
      "Epoch 52/150\n",
      "3066/3066 [==============================] - 1s 188us/sample - loss: 0.0065 - correlation_coefficient: 0.6156 - val_loss: 0.0034 - val_correlation_coefficient: 0.6302\n",
      "Epoch 53/150\n",
      "3066/3066 [==============================] - 1s 191us/sample - loss: 0.0065 - correlation_coefficient: 0.4273 - val_loss: 0.0032 - val_correlation_coefficient: 0.6332\n",
      "Epoch 54/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0065 - correlation_coefficient: 0.7198 - val_loss: 0.0032 - val_correlation_coefficient: 0.6320\n",
      "Epoch 55/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.7023 - val_loss: 0.0033 - val_correlation_coefficient: 0.6263\n",
      "Epoch 56/150\n",
      "3066/3066 [==============================] - 1s 218us/sample - loss: 0.0065 - correlation_coefficient: 0.6743 - val_loss: 0.0033 - val_correlation_coefficient: 0.6348\n",
      "Epoch 57/150\n",
      "3066/3066 [==============================] - 1s 216us/sample - loss: 0.0065 - correlation_coefficient: 0.7475 - val_loss: 0.0033 - val_correlation_coefficient: 0.6329\n",
      "Epoch 58/150\n",
      "3066/3066 [==============================] - 1s 225us/sample - loss: 0.0065 - correlation_coefficient: 0.6825 - val_loss: 0.0035 - val_correlation_coefficient: 0.6347\n",
      "Epoch 59/150\n",
      "3066/3066 [==============================] - 1s 199us/sample - loss: 0.0065 - correlation_coefficient: 0.5350 - val_loss: 0.0033 - val_correlation_coefficient: 0.6331\n",
      "Epoch 60/150\n",
      "3066/3066 [==============================] - 1s 198us/sample - loss: 0.0065 - correlation_coefficient: 0.7250 - val_loss: 0.0036 - val_correlation_coefficient: -0.6253\n",
      "Epoch 61/150\n",
      "3066/3066 [==============================] - 1s 210us/sample - loss: 0.0065 - correlation_coefficient: 0.7576 - val_loss: 0.0033 - val_correlation_coefficient: 0.6313\n",
      "Epoch 62/150\n",
      "3066/3066 [==============================] - 1s 208us/sample - loss: 0.0065 - correlation_coefficient: 0.5875 - val_loss: 0.0033 - val_correlation_coefficient: -0.6320\n",
      "Epoch 63/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0065 - correlation_coefficient: 0.5899 - val_loss: 0.0033 - val_correlation_coefficient: 0.6335\n",
      "Epoch 64/150\n",
      "3066/3066 [==============================] - 1s 222us/sample - loss: 0.0065 - correlation_coefficient: 0.6297 - val_loss: 0.0035 - val_correlation_coefficient: 0.6351\n",
      "Epoch 65/150\n",
      "3066/3066 [==============================] - 1s 189us/sample - loss: 0.0065 - correlation_coefficient: 0.4778 - val_loss: 0.0035 - val_correlation_coefficient: 0.6336\n",
      "Epoch 66/150\n",
      "3066/3066 [==============================] - 1s 186us/sample - loss: 0.0064 - correlation_coefficient: 0.6336 - val_loss: 0.0033 - val_correlation_coefficient: 0.6347\n",
      "Epoch 67/150\n",
      "3066/3066 [==============================] - 1s 193us/sample - loss: 0.0065 - correlation_coefficient: 0.5948 - val_loss: 0.0032 - val_correlation_coefficient: 0.6332\n",
      "Epoch 68/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0065 - correlation_coefficient: 0.6319 - val_loss: 0.0033 - val_correlation_coefficient: 0.6300\n",
      "Epoch 69/150\n",
      "3066/3066 [==============================] - 1s 203us/sample - loss: 0.0065 - correlation_coefficient: 0.6544 - val_loss: 0.0033 - val_correlation_coefficient: 0.6211\n",
      "Epoch 70/150\n",
      "3066/3066 [==============================] - 1s 213us/sample - loss: 0.0065 - correlation_coefficient: 0.5838 - val_loss: 0.0032 - val_correlation_coefficient: 0.6292\n",
      "Epoch 71/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.7483 - val_loss: 0.0033 - val_correlation_coefficient: 0.5983\n",
      "Epoch 72/150\n",
      "3066/3066 [==============================] - 1s 201us/sample - loss: 0.0064 - correlation_coefficient: 0.7163 - val_loss: 0.0035 - val_correlation_coefficient: 0.6206\n",
      "Epoch 73/150\n",
      "3066/3066 [==============================] - 1s 194us/sample - loss: 0.0065 - correlation_coefficient: 0.6126 - val_loss: 0.0033 - val_correlation_coefficient: 0.6306\n",
      "Epoch 74/150\n",
      "3066/3066 [==============================] - 1s 201us/sample - loss: 0.0065 - correlation_coefficient: 0.6976 - val_loss: 0.0033 - val_correlation_coefficient: 0.6331\n",
      "Epoch 75/150\n",
      "3066/3066 [==============================] - 1s 201us/sample - loss: 0.0065 - correlation_coefficient: 0.4613 - val_loss: 0.0034 - val_correlation_coefficient: -0.6332\n",
      "Epoch 76/150\n",
      "3066/3066 [==============================] - 1s 191us/sample - loss: 0.0065 - correlation_coefficient: 0.6482 - val_loss: 0.0034 - val_correlation_coefficient: 0.6349\n",
      "Epoch 77/150\n",
      "3066/3066 [==============================] - 1s 207us/sample - loss: 0.0065 - correlation_coefficient: 0.7505 - val_loss: 0.0032 - val_correlation_coefficient: 0.6294\n",
      "Epoch 78/150\n",
      "3066/3066 [==============================] - 1s 213us/sample - loss: 0.0065 - correlation_coefficient: 0.6841 - val_loss: 0.0033 - val_correlation_coefficient: 0.6323\n",
      "Epoch 79/150\n",
      "3066/3066 [==============================] - 1s 196us/sample - loss: 0.0065 - correlation_coefficient: 0.7523 - val_loss: 0.0039 - val_correlation_coefficient: -0.4690\n",
      "Epoch 80/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0065 - correlation_coefficient: 0.7479 - val_loss: 0.0036 - val_correlation_coefficient: 0.6036\n",
      "Epoch 81/150\n",
      "3066/3066 [==============================] - 1s 192us/sample - loss: 0.0065 - correlation_coefficient: 0.7038 - val_loss: 0.0033 - val_correlation_coefficient: 0.6342\n",
      "Epoch 82/150\n",
      "3066/3066 [==============================] - 1s 213us/sample - loss: 0.0065 - correlation_coefficient: 0.6357 - val_loss: 0.0032 - val_correlation_coefficient: 0.6269\n",
      "Epoch 83/150\n",
      "3066/3066 [==============================] - 1s 214us/sample - loss: 0.0065 - correlation_coefficient: 0.6252 - val_loss: 0.0036 - val_correlation_coefficient: 0.6239\n",
      "Epoch 84/150\n",
      "3066/3066 [==============================] - 1s 199us/sample - loss: 0.0065 - correlation_coefficient: 0.6691 - val_loss: 0.0033 - val_correlation_coefficient: 0.6313\n",
      "Epoch 85/150\n",
      "3066/3066 [==============================] - 1s 189us/sample - loss: 0.0065 - correlation_coefficient: 0.6686 - val_loss: 0.0032 - val_correlation_coefficient: 0.6270\n",
      "Epoch 86/150\n",
      "3066/3066 [==============================] - 1s 183us/sample - loss: 0.0065 - correlation_coefficient: 0.6287 - val_loss: 0.0033 - val_correlation_coefficient: 0.6349\n",
      "Epoch 87/150\n",
      "3066/3066 [==============================] - 1s 188us/sample - loss: 0.0065 - correlation_coefficient: 0.7229 - val_loss: 0.0034 - val_correlation_coefficient: 0.6271\n",
      "Epoch 88/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.6193 - val_loss: 0.0033 - val_correlation_coefficient: 0.6335\n",
      "Epoch 89/150\n",
      "3066/3066 [==============================] - 1s 185us/sample - loss: 0.0065 - correlation_coefficient: 0.7137 - val_loss: 0.0033 - val_correlation_coefficient: -0.6314\n",
      "Epoch 90/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0065 - correlation_coefficient: 0.7190 - val_loss: 0.0033 - val_correlation_coefficient: 0.6330\n",
      "Epoch 91/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066/3066 [==============================] - 1s 176us/sample - loss: 0.0064 - correlation_coefficient: 0.6168 - val_loss: 0.0034 - val_correlation_coefficient: 0.6273\n",
      "Epoch 92/150\n",
      "3066/3066 [==============================] - 1s 185us/sample - loss: 0.0065 - correlation_coefficient: 0.6240 - val_loss: 0.0033 - val_correlation_coefficient: 0.6347\n",
      "Epoch 93/150\n",
      "3066/3066 [==============================] - 1s 177us/sample - loss: 0.0065 - correlation_coefficient: 0.6449 - val_loss: 0.0033 - val_correlation_coefficient: 0.5926\n",
      "Epoch 94/150\n",
      "3066/3066 [==============================] - 1s 186us/sample - loss: 0.0065 - correlation_coefficient: 0.6460 - val_loss: 0.0032 - val_correlation_coefficient: 0.6325\n",
      "Epoch 95/150\n",
      "3066/3066 [==============================] - 1s 181us/sample - loss: 0.0065 - correlation_coefficient: 0.7697 - val_loss: 0.0034 - val_correlation_coefficient: 0.6333\n",
      "Epoch 96/150\n",
      "3066/3066 [==============================] - 1s 184us/sample - loss: 0.0065 - correlation_coefficient: 0.5099 - val_loss: 0.0033 - val_correlation_coefficient: 0.6343\n",
      "Epoch 97/150\n",
      "3066/3066 [==============================] - 1s 184us/sample - loss: 0.0065 - correlation_coefficient: 0.6527 - val_loss: 0.0033 - val_correlation_coefficient: 0.6092\n",
      "Epoch 98/150\n",
      "3066/3066 [==============================] - 1s 171us/sample - loss: 0.0065 - correlation_coefficient: 0.6005 - val_loss: 0.0032 - val_correlation_coefficient: 0.6333\n",
      "Epoch 99/150\n",
      "3066/3066 [==============================] - 1s 210us/sample - loss: 0.0065 - correlation_coefficient: 0.6059 - val_loss: 0.0033 - val_correlation_coefficient: 0.6296\n",
      "Epoch 100/150\n",
      "3066/3066 [==============================] - 1s 191us/sample - loss: 0.0064 - correlation_coefficient: 0.6465 - val_loss: 0.0035 - val_correlation_coefficient: 0.6352\n",
      "Epoch 101/150\n",
      "3066/3066 [==============================] - 1s 197us/sample - loss: 0.0065 - correlation_coefficient: 0.7159 - val_loss: 0.0032 - val_correlation_coefficient: 0.6235\n",
      "Epoch 102/150\n",
      "3066/3066 [==============================] - 1s 194us/sample - loss: 0.0065 - correlation_coefficient: 0.7511 - val_loss: 0.0036 - val_correlation_coefficient: 0.6339\n",
      "Epoch 103/150\n",
      "3066/3066 [==============================] - 1s 187us/sample - loss: 0.0065 - correlation_coefficient: 0.6856 - val_loss: 0.0035 - val_correlation_coefficient: 0.6307\n",
      "Epoch 104/150\n",
      "3066/3066 [==============================] - 1s 197us/sample - loss: 0.0065 - correlation_coefficient: 0.7968 - val_loss: 0.0032 - val_correlation_coefficient: 0.6338\n",
      "Epoch 105/150\n",
      "3066/3066 [==============================] - 1s 186us/sample - loss: 0.0065 - correlation_coefficient: 0.6807 - val_loss: 0.0034 - val_correlation_coefficient: 0.6331\n",
      "Epoch 106/150\n",
      "3066/3066 [==============================] - 1s 190us/sample - loss: 0.0064 - correlation_coefficient: 0.6266 - val_loss: 0.0036 - val_correlation_coefficient: 0.6304\n",
      "Epoch 107/150\n",
      "3066/3066 [==============================] - 1s 211us/sample - loss: 0.0065 - correlation_coefficient: 0.7127 - val_loss: 0.0034 - val_correlation_coefficient: 0.6338\n",
      "Epoch 108/150\n",
      "3066/3066 [==============================] - 1s 213us/sample - loss: 0.0065 - correlation_coefficient: 0.6302 - val_loss: 0.0033 - val_correlation_coefficient: 0.6322\n",
      "Epoch 109/150\n",
      "3066/3066 [==============================] - 1s 280us/sample - loss: 0.0065 - correlation_coefficient: 0.6090 - val_loss: 0.0038 - val_correlation_coefficient: 0.6226\n",
      "Epoch 110/150\n",
      "3066/3066 [==============================] - 1s 243us/sample - loss: 0.0065 - correlation_coefficient: 0.8133 - val_loss: 0.0037 - val_correlation_coefficient: 0.6302\n",
      "Epoch 111/150\n",
      "3066/3066 [==============================] - 1s 270us/sample - loss: 0.0065 - correlation_coefficient: 0.8207 - val_loss: 0.0033 - val_correlation_coefficient: 0.6332\n",
      "Epoch 112/150\n",
      "3066/3066 [==============================] - 1s 193us/sample - loss: 0.0065 - correlation_coefficient: 0.7575 - val_loss: 0.0034 - val_correlation_coefficient: 0.6173\n",
      "Epoch 113/150\n",
      "3066/3066 [==============================] - 1s 212us/sample - loss: 0.0065 - correlation_coefficient: 0.7170 - val_loss: 0.0032 - val_correlation_coefficient: 0.6335\n",
      "Epoch 114/150\n",
      "3066/3066 [==============================] - 1s 209us/sample - loss: 0.0065 - correlation_coefficient: 0.6784 - val_loss: 0.0033 - val_correlation_coefficient: 0.6343\n",
      "Epoch 115/150\n",
      "3066/3066 [==============================] - 1s 210us/sample - loss: 0.0065 - correlation_coefficient: 0.6270 - val_loss: 0.0033 - val_correlation_coefficient: 0.6236\n",
      "Epoch 116/150\n",
      "3066/3066 [==============================] - 1s 233us/sample - loss: 0.0065 - correlation_coefficient: 0.5668 - val_loss: 0.0032 - val_correlation_coefficient: 0.6308\n",
      "Epoch 117/150\n",
      "3066/3066 [==============================] - 1s 211us/sample - loss: 0.0065 - correlation_coefficient: 0.6758 - val_loss: 0.0036 - val_correlation_coefficient: -0.6334\n",
      "Epoch 118/150\n",
      "3066/3066 [==============================] - 1s 206us/sample - loss: 0.0065 - correlation_coefficient: 0.5675 - val_loss: 0.0036 - val_correlation_coefficient: -0.5790\n",
      "Epoch 119/150\n",
      "3066/3066 [==============================] - 1s 224us/sample - loss: 0.0065 - correlation_coefficient: 0.5857 - val_loss: 0.0035 - val_correlation_coefficient: -0.3643\n",
      "Epoch 120/150\n",
      "3066/3066 [==============================] - 1s 202us/sample - loss: 0.0065 - correlation_coefficient: 0.7002 - val_loss: 0.0033 - val_correlation_coefficient: 0.5976\n",
      "Epoch 121/150\n",
      "3066/3066 [==============================] - 1s 215us/sample - loss: 0.0065 - correlation_coefficient: 0.6428 - val_loss: 0.0034 - val_correlation_coefficient: 0.6266\n",
      "Epoch 122/150\n",
      "3066/3066 [==============================] - 1s 221us/sample - loss: 0.0065 - correlation_coefficient: 0.7621 - val_loss: 0.0034 - val_correlation_coefficient: 0.5987\n",
      "Epoch 123/150\n",
      "3066/3066 [==============================] - 1s 203us/sample - loss: 0.0065 - correlation_coefficient: 0.6502 - val_loss: 0.0035 - val_correlation_coefficient: 0.6348\n",
      "Epoch 124/150\n",
      "3066/3066 [==============================] - 1s 216us/sample - loss: 0.0065 - correlation_coefficient: 0.6474 - val_loss: 0.0034 - val_correlation_coefficient: -0.6288\n",
      "Epoch 125/150\n",
      "3066/3066 [==============================] - 1s 224us/sample - loss: 0.0065 - correlation_coefficient: 0.6201 - val_loss: 0.0034 - val_correlation_coefficient: 0.6241\n",
      "Epoch 126/150\n",
      "3066/3066 [==============================] - 1s 208us/sample - loss: 0.0065 - correlation_coefficient: 0.5953 - val_loss: 0.0034 - val_correlation_coefficient: -0.6290\n",
      "Epoch 127/150\n",
      "3066/3066 [==============================] - 1s 228us/sample - loss: 0.0065 - correlation_coefficient: 0.7022 - val_loss: 0.0033 - val_correlation_coefficient: 0.6329\n",
      "Epoch 128/150\n",
      "3066/3066 [==============================] - 1s 208us/sample - loss: 0.0065 - correlation_coefficient: 0.6781 - val_loss: 0.0034 - val_correlation_coefficient: 0.6313\n",
      "Epoch 129/150\n",
      "3066/3066 [==============================] - 1s 218us/sample - loss: 0.0065 - correlation_coefficient: 0.7446 - val_loss: 0.0032 - val_correlation_coefficient: 0.6315\n",
      "Epoch 130/150\n",
      "3066/3066 [==============================] - 1s 205us/sample - loss: 0.0065 - correlation_coefficient: 0.6671 - val_loss: 0.0036 - val_correlation_coefficient: 0.6346\n",
      "Epoch 131/150\n",
      "3066/3066 [==============================] - 1s 209us/sample - loss: 0.0065 - correlation_coefficient: 0.6530 - val_loss: 0.0034 - val_correlation_coefficient: -0.6320\n",
      "Epoch 132/150\n",
      "3066/3066 [==============================] - 1s 199us/sample - loss: 0.0065 - correlation_coefficient: 0.7062 - val_loss: 0.0033 - val_correlation_coefficient: 0.6132\n",
      "Epoch 133/150\n",
      "3066/3066 [==============================] - 1s 232us/sample - loss: 0.0065 - correlation_coefficient: 0.7061 - val_loss: 0.0033 - val_correlation_coefficient: 0.6346\n",
      "Epoch 134/150\n",
      "3066/3066 [==============================] - 1s 230us/sample - loss: 0.0065 - correlation_coefficient: 0.6459 - val_loss: 0.0033 - val_correlation_coefficient: 0.6258\n",
      "Epoch 135/150\n",
      "3066/3066 [==============================] - 1s 218us/sample - loss: 0.0065 - correlation_coefficient: 0.7324 - val_loss: 0.0033 - val_correlation_coefficient: 0.6325\n",
      "Epoch 136/150\n",
      "3066/3066 [==============================] - 1s 210us/sample - loss: 0.0065 - correlation_coefficient: 0.6394 - val_loss: 0.0033 - val_correlation_coefficient: 0.6320\n",
      "Epoch 137/150\n",
      "3066/3066 [==============================] - 1s 214us/sample - loss: 0.0064 - correlation_coefficient: 0.7190 - val_loss: 0.0033 - val_correlation_coefficient: -0.5840\n",
      "Epoch 138/150\n",
      "3066/3066 [==============================] - 1s 192us/sample - loss: 0.0065 - correlation_coefficient: 0.7508 - val_loss: 0.0033 - val_correlation_coefficient: 0.6336\n",
      "Epoch 139/150\n",
      "3066/3066 [==============================] - 1s 204us/sample - loss: 0.0065 - correlation_coefficient: 0.7363 - val_loss: 0.0033 - val_correlation_coefficient: 0.6320\n",
      "Epoch 140/150\n",
      "3066/3066 [==============================] - 1s 209us/sample - loss: 0.0065 - correlation_coefficient: 0.6698 - val_loss: 0.0035 - val_correlation_coefficient: 0.6200\n",
      "Epoch 141/150\n",
      "3066/3066 [==============================] - 1s 193us/sample - loss: 0.0065 - correlation_coefficient: 0.5548 - val_loss: 0.0033 - val_correlation_coefficient: 0.6341\n",
      "Epoch 142/150\n",
      "3066/3066 [==============================] - 1s 212us/sample - loss: 0.0065 - correlation_coefficient: 0.6633 - val_loss: 0.0034 - val_correlation_coefficient: 0.6343\n",
      "Epoch 143/150\n",
      "3066/3066 [==============================] - 1s 197us/sample - loss: 0.0065 - correlation_coefficient: 0.7486 - val_loss: 0.0033 - val_correlation_coefficient: 0.6343\n",
      "Epoch 144/150\n",
      "3066/3066 [==============================] - 1s 207us/sample - loss: 0.0065 - correlation_coefficient: 0.5820 - val_loss: 0.0032 - val_correlation_coefficient: 0.6339\n",
      "Epoch 145/150\n",
      "3066/3066 [==============================] - 1s 211us/sample - loss: 0.0064 - correlation_coefficient: 0.6567 - val_loss: 0.0035 - val_correlation_coefficient: 0.6310\n",
      "Epoch 146/150\n",
      "3066/3066 [==============================] - 1s 192us/sample - loss: 0.0065 - correlation_coefficient: 0.6723 - val_loss: 0.0034 - val_correlation_coefficient: -0.6246\n",
      "Epoch 147/150\n",
      "3066/3066 [==============================] - 1s 206us/sample - loss: 0.0065 - correlation_coefficient: 0.7783 - val_loss: 0.0038 - val_correlation_coefficient: -0.6345\n",
      "Epoch 148/150\n",
      "3066/3066 [==============================] - 1s 203us/sample - loss: 0.0065 - correlation_coefficient: 0.6865 - val_loss: 0.0035 - val_correlation_coefficient: 0.6337\n",
      "Epoch 149/150\n",
      "3066/3066 [==============================] - 1s 196us/sample - loss: 0.0065 - correlation_coefficient: 0.7058 - val_loss: 0.0034 - val_correlation_coefficient: -0.6360\n",
      "Epoch 150/150\n",
      "3066/3066 [==============================] - 1s 216us/sample - loss: 0.0064 - correlation_coefficient: 0.8137 - val_loss: 0.0035 - val_correlation_coefficient: 0.6334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2510dc6f488>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.fit(x = [x_Train, y_Train[:, :-1]], y = y_Train[:, 1:],\n",
    "               batch_size=32,epochs=150,validation_data=([x_Test, y_Test[:, :-1]], y_Test[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
